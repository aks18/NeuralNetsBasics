{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Working on Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "#importing libraries to be used\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the mnist dataset from the keras library\n",
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow._api.v1.keras.datasets.fashion_mnist' from '/home/shitbot009/anaconda3/lib/python3.7/site-packages/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training data and labels and test data and labels. These are predefined in the MNIST dataset\n",
    "(trn_imgs, trn_lbls), (tst_imgs, tst_lbls) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01176471\n",
      "  0.         0.         0.14117647 0.2627451  0.21960784 0.\n",
      "  0.         0.01176471 0.         0.         0.         0.\n",
      "  0.         0.         0.15686275 0.10588235]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.00784314 0.         0.         0.\n",
      "  0.         0.62745098 1.         0.85098039 1.         0.36862745\n",
      "  0.         0.         0.         0.00392157 0.01568627 0.\n",
      "  0.         0.         0.25490196 0.14901961]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.00392157 0.         0.         0.         0.         0.58823529\n",
      "  0.9254902  0.84313725 0.78431373 0.76470588 0.77254902 0.98431373\n",
      "  0.77647059 0.16078431 0.         0.         0.         0.\n",
      "  0.         0.24705882 0.9254902  0.10196078]\n",
      " [0.         0.00784314 0.00392157 0.         0.         0.\n",
      "  0.         0.         0.25882353 0.65490196 0.94901961 0.84705882\n",
      "  0.8        0.74509804 0.74901961 0.78823529 0.78039216 0.77254902\n",
      "  0.83529412 1.         0.76862745 0.47843137 0.37647059 0.47843137\n",
      "  0.72941176 0.80784314 0.89019608 0.24705882]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.16078431 0.69803922 0.87843137 0.83529412 0.78039216 0.75294118\n",
      "  0.75686275 0.74901961 0.75294118 0.78039216 0.80784314 0.80392157\n",
      "  0.79607843 0.80784314 0.83921569 0.88235294 0.88627451 0.87843137\n",
      "  0.85882353 0.8        0.85098039 0.49411765]\n",
      " [0.         0.29019608 0.5254902  0.57254902 0.65098039 0.74901961\n",
      "  0.8        0.78431373 0.75686275 0.74509804 0.75686275 0.76862745\n",
      "  0.75686275 0.79215686 0.8        0.81176471 0.80392157 0.78431373\n",
      "  0.79607843 0.80392157 0.79215686 0.78823529 0.78039216 0.77647059\n",
      "  0.78431373 0.77254902 0.86666667 0.51764706]\n",
      " [0.04705882 0.64313725 0.83921569 0.85098039 0.8627451  0.84705882\n",
      "  0.84313725 0.81960784 0.82745098 0.81568627 0.81176471 0.80392157\n",
      "  0.81176471 0.81960784 0.80784314 0.80784314 0.80784314 0.80784314\n",
      "  0.81568627 0.81176471 0.81568627 0.80784314 0.80784314 0.81176471\n",
      "  0.81176471 0.79607843 0.88627451 0.58823529]\n",
      " [0.10980392 0.00784314 0.         0.0627451  0.16862745 0.32156863\n",
      "  0.46666667 0.5254902  0.6627451  0.7254902  0.80784314 0.85882353\n",
      "  0.8745098  0.89411765 0.91372549 0.93333333 0.94901961 0.94509804\n",
      "  0.93333333 0.92156863 0.92941176 0.92941176 0.9254902  0.91372549\n",
      "  0.89411765 0.84705882 0.96470588 0.40784314]\n",
      " [0.12941176 0.21960784 0.16862745 0.0745098  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.03137255 0.0627451  0.05098039 0.07058824\n",
      "  0.08235294 0.05098039 0.04313725 0.05882353 0.07843137 0.07843137\n",
      "  0.08627451 0.08235294 0.23529412 0.14509804]\n",
      " [0.         0.00392157 0.05098039 0.16078431 0.29411765 0.31764706\n",
      "  0.26666667 0.2627451  0.22352941 0.18431373 0.14901961 0.14901961\n",
      "  0.1254902  0.1372549  0.14901961 0.1372549  0.14509804 0.16078431\n",
      "  0.14509804 0.14117647 0.14901961 0.15686275 0.18431373 0.20784314\n",
      "  0.23921569 0.30980392 0.44705882 0.24705882]\n",
      " [0.         0.         0.         0.         0.00392157 0.05882353\n",
      "  0.12156863 0.21960784 0.25882353 0.26666667 0.32156863 0.35294118\n",
      "  0.31764706 0.27843137 0.24313725 0.18039216 0.13333333 0.12941176\n",
      "  0.14901961 0.17647059 0.18039216 0.16862745 0.11764706 0.0745098\n",
      "  0.05490196 0.06666667 0.08627451 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#printing to see one of the images in the training dataset along with its labels\n",
    "plt.imshow(trn_imgs[20000])\n",
    "print(trn_lbls[20000])\n",
    "print(trn_imgs[20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Input Data\n",
    "All values are between 0 and 255. While training a neural network its better if we **normalize** our data i.e.\n",
    "convert all of it between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this converts all of the images values of pixels in between 0 and 1\n",
    "trn_imgs = trn_imgs / 255.0\n",
    "tst_imgs = tst_imgs / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00392157 0.         0.\n",
      "  0.07058824 0.41960784 0.46666667 0.40392157 0.03529412 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.38823529\n",
      "  0.60784314 0.44313725 0.23921569 0.4627451  0.67843137 0.45882353\n",
      "  0.         0.         0.01176471 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.00392157 0.         0.43137255 0.53333333\n",
      "  0.         0.         0.         0.         0.         0.65490196\n",
      "  0.62352941 0.         0.         0.00784314 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.00392157\n",
      "  0.         0.02352941 0.         0.28235294 0.57647059 0.\n",
      "  0.         0.01960784 0.         0.00784314 0.         0.\n",
      "  0.68235294 0.4627451  0.         0.01960784 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.00392157\n",
      "  0.01960784 0.         0.         0.68235294 0.01568627 0.\n",
      "  0.01568627 0.         0.         0.         0.         0.\n",
      "  0.         0.8        0.17254902 0.         0.01568627 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.03137255 0.         0.49019608 0.50196078 0.         0.03529412\n",
      "  0.         0.         0.         0.         0.         0.00784314\n",
      "  0.         0.41960784 0.59607843 0.         0.02352941 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.72156863 0.03529412 0.         0.01568627\n",
      "  0.         0.         0.         0.         0.         0.00784314\n",
      "  0.         0.         0.74509804 0.         0.         0.00784314\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00392157 0.         0.00784314\n",
      "  0.         0.28627451 0.61960784 0.         0.         0.\n",
      "  0.         0.00392157 0.         0.         0.         0.\n",
      "  0.01568627 0.         0.61568627 0.30980392 0.         0.01176471\n",
      "  0.00392157 0.         0.         0.        ]\n",
      " [0.         0.         0.00784314 0.01960784 0.00392157 0.\n",
      "  0.         0.61568627 0.38431373 0.         0.00784314 0.\n",
      "  0.         0.00392157 0.         0.         0.         0.\n",
      "  0.01960784 0.         0.4745098  0.56862745 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.90588235 0.32941176 0.         0.00784314 0.00392157\n",
      "  0.00392157 0.         0.         0.         0.         0.\n",
      "  0.00392157 0.         0.36078431 0.8745098  0.0627451  0.\n",
      "  0.02745098 0.01568627 0.00392157 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.14901961 0.71764706 0.31764706 0.         0.         0.\n",
      "  0.00784314 0.00784314 0.00392157 0.00784314 0.00392157 0.\n",
      "  0.00392157 0.         0.47058824 0.88627451 0.22352941 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.67843137 0.84705882 0.75686275 0.83529412\n",
      "  0.71764706 0.64313725 0.65490196 0.38431373 0.0627451  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.41960784 0.55294118 0.43529412 0.35686275\n",
      "  0.35294118 0.42352941 0.19607843 0.        ]\n",
      " [0.         0.         0.7254902  0.86666667 0.85098039 0.82352941\n",
      "  0.79215686 0.87058824 0.78431373 0.80784314 0.79215686 0.8\n",
      "  0.42745098 0.10588235 0.04705882 0.06666667 0.23921569 0.53333333\n",
      "  0.70588235 0.86666667 0.79215686 0.88235294 0.81568627 0.83921569\n",
      "  0.8745098  0.94117647 0.63529412 0.        ]\n",
      " [0.         0.         0.71764706 0.8627451  0.80392157 0.78431373\n",
      "  0.75686275 0.72156863 0.74117647 0.71372549 0.67843137 0.76078431\n",
      "  0.84313725 0.84705882 0.80392157 0.81176471 0.81176471 0.76470588\n",
      "  0.7254902  0.76078431 0.76078431 0.79607843 0.83137255 0.71764706\n",
      "  0.75686275 0.82745098 0.6        0.        ]\n",
      " [0.         0.         0.74509804 0.91372549 0.79607843 0.80784314\n",
      "  0.83921569 0.84705882 0.76470588 0.71764706 0.69019608 0.62352941\n",
      "  0.68627451 0.74117647 0.79215686 0.76470588 0.72941176 0.72941176\n",
      "  0.71372549 0.72941176 0.76470588 0.81960784 0.92156863 0.79607843\n",
      "  0.79215686 0.84313725 0.53333333 0.        ]\n",
      " [0.         0.         0.70588235 0.89411765 0.78823529 0.79215686\n",
      "  0.7372549  0.71764706 0.69803922 0.75294118 0.72941176 0.70980392\n",
      "  0.70980392 0.69411765 0.8        0.85098039 0.68627451 0.70196078\n",
      "  0.72156863 0.69019608 0.65098039 0.6627451  0.68627451 0.7254902\n",
      "  0.59607843 0.79607843 0.41960784 0.        ]\n",
      " [0.         0.         0.65490196 0.92156863 0.78431373 0.81176471\n",
      "  0.81176471 0.81960784 0.81176471 0.79607843 0.77647059 0.74117647\n",
      "  0.71372549 0.69803922 0.72941176 0.74117647 0.69803922 0.75294118\n",
      "  0.77254902 0.76470588 0.75294118 0.7254902  0.72156863 0.80392157\n",
      "  0.73333333 1.         0.23921569 0.        ]\n",
      " [0.         0.         0.59607843 0.98039216 0.81568627 0.83921569\n",
      "  0.81960784 0.79215686 0.78431373 0.79215686 0.80392157 0.8\n",
      "  0.78431373 0.75294118 0.77647059 0.8        0.76470588 0.80392157\n",
      "  0.80784314 0.77647059 0.78039216 0.79607843 0.83921569 0.81176471\n",
      "  0.70196078 0.97647059 0.21568627 0.        ]\n",
      " [0.         0.         0.49411765 1.         0.82745098 0.84313725\n",
      "  0.82352941 0.80784314 0.79607843 0.79607843 0.79607843 0.80784314\n",
      "  0.8        0.78039216 0.80784314 0.81176471 0.78431373 0.80784314\n",
      "  0.78823529 0.78039216 0.79215686 0.81568627 0.84313725 0.76470588\n",
      "  0.68235294 0.82745098 0.05882353 0.        ]\n",
      " [0.         0.         0.41960784 1.         0.83137255 0.82745098\n",
      "  0.82352941 0.82745098 0.81568627 0.80784314 0.80784314 0.81176471\n",
      "  0.81568627 0.81568627 0.85098039 0.83921569 0.80392157 0.80784314\n",
      "  0.8        0.81568627 0.82745098 0.82745098 0.8627451  0.77254902\n",
      "  0.71372549 0.88235294 0.         0.        ]\n",
      " [0.         0.         0.24705882 0.90980392 0.82745098 0.84705882\n",
      "  0.83137255 0.83921569 0.83137255 0.83137255 0.83921569 0.83137255\n",
      "  0.82745098 0.83137255 0.8745098  0.85882353 0.82352941 0.83921569\n",
      "  0.83529412 0.83137255 0.82745098 0.81960784 0.85098039 0.81568627\n",
      "  0.65882353 0.74509804 0.         0.        ]\n",
      " [0.         0.         0.05490196 1.         0.85490196 0.86666667\n",
      "  0.84313725 0.85490196 0.85490196 0.85098039 0.84313725 0.83529412\n",
      "  0.82745098 0.84313725 0.89411765 0.8627451  0.84705882 0.87058824\n",
      "  0.85098039 0.85098039 0.84705882 0.85490196 0.83137255 0.83529412\n",
      "  0.74117647 0.56078431 0.         0.        ]\n",
      " [0.         0.         0.         0.90196078 0.89019608 0.83921569\n",
      "  0.85098039 0.85490196 0.84705882 0.85098039 0.85098039 0.83921569\n",
      "  0.84705882 0.87058824 0.88627451 0.85490196 0.85490196 0.8627451\n",
      "  0.85882353 0.85490196 0.85098039 0.85490196 0.85098039 0.84313725\n",
      "  0.84705882 0.44313725 0.         0.        ]\n",
      " [0.         0.         0.         0.69411765 0.91372549 0.84705882\n",
      "  0.8745098  0.87058824 0.83529412 0.83529412 0.84313725 0.83137255\n",
      "  0.8627451  0.88627451 0.86666667 0.8627451  0.86666667 0.85490196\n",
      "  0.87058824 0.8627451  0.8627451  0.86666667 0.85098039 0.83137255\n",
      "  0.85882353 0.20392157 0.         0.        ]\n",
      " [0.         0.         0.         0.03921569 0.8745098  0.87058824\n",
      "  0.8627451  0.82745098 0.83529412 0.84313725 0.8627451  0.87058824\n",
      "  0.88627451 0.88627451 0.86666667 0.8745098  0.87058824 0.87058824\n",
      "  0.86666667 0.85490196 0.8627451  0.85882353 0.8627451  0.8745098\n",
      "  0.68235294 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.49411765 0.94509804\n",
      "  0.81568627 0.82352941 0.83921569 0.83921569 0.84705882 0.84705882\n",
      "  0.8627451  0.8627451  0.83921569 0.83529412 0.83137255 0.83137255\n",
      "  0.82352941 0.84313725 0.85098039 0.85490196 0.84313725 0.9254902\n",
      "  0.16470588 0.         0.00392157 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.7372549\n",
      "  0.92941176 0.90196078 0.91372549 1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         0.90588235 0.89803922 0.9372549  0.63137255\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.24705882 0.37254902 0.42352941 0.41176471 0.39215686 0.40392157\n",
      "  0.40784314 0.41176471 0.4        0.39215686 0.38431373 0.38039216\n",
      "  0.37647059 0.35294118 0.31372549 0.3254902  0.23529412 0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#can be seen here, all pixel values in between 0 and 1\n",
    "plt.imshow(trn_imgs[100])\n",
    "print(trn_lbls[100])\n",
    "print(trn_imgs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture of the neural network model.\n",
    "#3 layers - 1st: Flatten, for input; 2nd: Dense(hidden layer 1), with 128 neurons, and ReLU activation function\n",
    "#and 3rd: Dense, for output, with 10 neurons (because output must be classified into 10 types), and activation\n",
    "#function - SOFTMAX\n",
    "#all these 3 layers are sequentially placed\n",
    "nn_model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
    "                                       tf.keras.layers.Dense(200, activation = tf.nn.relu),\n",
    "                                       tf.keras.layers.Dense(10, activation = tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the neural network model, specifying which optimizer to use, which loss function to use & which \n",
    "#metrics to take care of\n",
    "nn_model.compile(optimizer = tf.train.AdamOptimizer(), \n",
    "                 loss = 'sparse_categorical_crossentropy', \n",
    "                 metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0672 - acc: 0.9744\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0666 - acc: 0.9745\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0649 - acc: 0.9757\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.0639 - acc: 0.9761\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0646 - acc: 0.9754\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.0609 - acc: 0.9776\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0614 - acc: 0.9765\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0598 - acc: 0.9775\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.0625 - acc: 0.9769\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0595 - acc: 0.9776\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0579 - acc: 0.9782\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0570 - acc: 0.9781\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.0560 - acc: 0.9798\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0573 - acc: 0.9790\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0550 - acc: 0.9794\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.0558 - acc: 0.9791\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0534 - acc: 0.9797\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0537 - acc: 0.9797\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0509 - acc: 0.9811\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.0540 - acc: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9b203a2898>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the model to the training data. Or simply put, starting the training and specifying the number of epochs\n",
    "nn_model.fit(trn_imgs, trn_lbls, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.5580 - acc: 0.8929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5579735549584031, 0.8929]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluating the model on test data\n",
    "nn_model.evaluate(tst_imgs, tst_lbls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model average **88.5%** accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.1297544e-18 1.7036587e-33 9.9823091e-17 1.1592602e-21 2.4499668e-33\n",
      " 4.4201497e-11 7.2617910e-25 9.2383338e-08 1.4109034e-23 9.9999988e-01]\n"
     ]
    }
   ],
   "source": [
    "classifications = nn_model.predict(tst_imgs)\n",
    "print(classifications[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(tst_lbls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4677448e-07 3.6562549e-19 1.8687748e-03 1.6020157e-20 9.9813002e-01\n",
      " 3.0774831e-12 9.4180376e-07 1.0469659e-14 8.6464280e-18 6.2309741e-10]\n"
     ]
    }
   ],
   "source": [
    "print(classifications[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(tst_lbls[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the classifications stores a list of probabilities for each item, that it falls in which class. And when printing the tst_lbls(test_labels), we can see that the label corresponds to the classification's highest probability element.\n",
    "In 1st case, the 9th element of classification array has the highest probability, and the label of that image is 9 (starting index 0). Similarly in the 2nd case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 28s 465us/sample - loss: 0.1850 - acc: 0.9439\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 29s 476us/sample - loss: 0.0752 - acc: 0.9769\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 28s 475us/sample - loss: 0.0475 - acc: 0.9854\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 28s 470us/sample - loss: 0.0349 - acc: 0.9885\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 28s 467us/sample - loss: 0.0261 - acc: 0.9914\n",
      "10000/10000 [==============================] - 1s 96us/sample - loss: 0.0763 - acc: 0.9770\n",
      "[1.6785570e-08 2.1979599e-10 2.5045094e-11 1.3164149e-11 4.6054985e-10\n",
      " 3.3319690e-08 1.0000000e+00 1.4182247e-10 5.5328342e-09 2.8986214e-12]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "#Loading the data\n",
    "mnist_numbers = tf.keras.datasets.mnist\n",
    "\n",
    "#loading the training data and test data into different variables\n",
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = mnist_numbers.load_data()\n",
    "\n",
    "#mapping the pixel values between 0 and 1\n",
    "train_imgs = train_imgs/255\n",
    "test_imgs = test_imgs/255\n",
    "\n",
    "#defining the model architecture\n",
    "mnist_numbers_model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
    "                                                 tf.keras.layers.Dense(1024, activation = tf.nn.relu), \n",
    "                                                 tf.keras.layers.Dense(10, activation = tf.nn.softmax)])\n",
    "\n",
    "#compiling the model, specifying the optimizer(which version of gradient descent to use), and the loss function\n",
    "mnist_numbers_model.compile(optimizer = 'adam', \n",
    "                           loss = 'sparse_categorical_crossentropy',\n",
    "                           metrics = ['accuracy'])\n",
    "\n",
    "#training the model\n",
    "mnist_numbers_model.fit(train_imgs, train_lbls, epochs = 5)\n",
    "\n",
    "#testing the model\n",
    "mnist_numbers_model.evaluate(test_imgs, test_lbls)\n",
    "\n",
    "#using the trained model to predict the classification of test_images into different classes\n",
    "classifications_mnist = mnist_numbers_model.predict(test_imgs)\n",
    "\n",
    "#Printing the classification probability array for 100th image of test_images and its label to verify\n",
    "print(classifications_mnist[100])\n",
    "print(test_lbls[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "Epoch 1/5\n",
      "50816/60000 [========================>.....] - ETA: 0s - loss: 0.3184"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-2ebaf9477b68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m               loss = 'sparse_categorical_crossentropy')\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "#Keeping just 5 neurons in the last layer gives and error as below\n",
    "#InvalidArgumentError: Received a label value of 9 which is outside the valid range of [0, 5).  Label values: 1 1 2 6 1 7 2 9 8 1 3 8 2 3 3 7 1 2 1 2 2 2 1 4 5 8 5 4 5 2 8 2\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    #tf.keras.layers.Dense(5, activation=tf.nn.softmax)])\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Implementing **Callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "WARNING:tensorflow:From /home/shitbot009/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.2613 - acc: 0.9252\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.1117 - acc: 0.9673\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0764 - acc: 0.9764\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0561 - acc: 0.9826\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.0436 - acc: 0.9865\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0347 - acc: 0.9888\n",
      "Epoch 7/20\n",
      "59936/60000 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9920\n",
      "Reached Accuracy Threshold, so stopping training.\n",
      "60000/60000 [==============================] - 26s 433us/sample - loss: 0.0262 - acc: 0.9920\n",
      "10000/10000 [==============================] - 2s 195us/sample - loss: 0.0762 - acc: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0762003939972259, 0.9773]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "ACCURACY_THRESHOLD = 0.99\n",
    "\n",
    "class stopping_callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('acc') > ACCURACY_THRESHOLD):\n",
    "            print(\"\\nReached Accuracy Threshold, so stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks1 = stopping_callback()\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = mnist.load_data()\n",
    "\n",
    "train_imgs = train_imgs/255.0\n",
    "test_imgs = test_imgs/255.0\n",
    "\n",
    "mnist_model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                         tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                         tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "mnist_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "mnist_model.fit(train_imgs, train_lbls, epochs=20, callbacks=[callbacks1])\n",
    "\n",
    "mnist_model.evaluate(test_imgs, test_lbls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
